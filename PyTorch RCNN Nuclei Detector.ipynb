{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import utils\n",
    "from utils.config import Config\n",
    "from data.dataset import Dataset, TestDataset, inverse_normalize\n",
    "from model import FasterRCNNVGG16\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data as data_\n",
    "from trainer import FasterRCNNTrainer\n",
    "from utils import array_tool as at\n",
    "from utils.vis_tool import visdom_bbox\n",
    "from utils.eval_tool import eval_detection_voc\n",
    "\n",
    "# fix for ulimit\n",
    "# https://github.com/pytorch/pytorch/issues/973#issuecomment-346405667\n",
    "import resource\n",
    "\n",
    "rlimit = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (20480, rlimit[1]))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix for ulimit\n",
    "# https://github.com/pytorch/pytorch/issues/973#issuecomment-346405667\n",
    "import resource\n",
    "\n",
    "rlimit = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (20480, rlimit[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(dataloader, faster_rcnn, test_num=10000):\n",
    "    pred_bboxes, pred_labels, pred_scores = list(), list(), list()\n",
    "    gt_bboxes, gt_labels, gt_difficults = list(), list(), list()\n",
    "    for ii, (imgs, sizes, gt_bboxes_, gt_labels_, gt_difficults_) in tqdm(enumerate(dataloader)):\n",
    "        sizes = [sizes[0][0], sizes[1][0]]\n",
    "        pred_bboxes_, pred_labels_, pred_scores_ = faster_rcnn.predict(imgs, [sizes])\n",
    "        gt_bboxes += list(gt_bboxes_.numpy())\n",
    "        gt_labels += list(gt_labels_.numpy())\n",
    "        gt_difficults += list(gt_difficults_.numpy())\n",
    "        pred_bboxes += pred_bboxes_\n",
    "        pred_labels += pred_labels_\n",
    "        pred_scores += pred_scores_\n",
    "        if ii == test_num: break\n",
    "\n",
    "    result = eval_detection_voc(\n",
    "        pred_bboxes, pred_labels, pred_scores,\n",
    "        gt_bboxes, gt_labels, gt_difficults,\n",
    "        use_07_metric=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(opt):\n",
    "    dataset = Dataset(opt, 'stage1_train_data.csv')\n",
    "    print('load data')\n",
    "    dataloader = data_.DataLoader(dataset, \\\n",
    "                                  batch_size=1, \\\n",
    "                                  shuffle=True, \\\n",
    "                                  # pin_memory=True,\n",
    "                                  num_workers=opt.num_workers)\n",
    "    testset = TestDataset(opt, 'stage1_val_data.csv')\n",
    "    test_dataloader = data_.DataLoader(testset,\n",
    "                                       batch_size=1,\n",
    "                                       num_workers=opt.test_num_workers,\n",
    "                                       shuffle=False, \\\n",
    "                                       pin_memory=True\n",
    "                                       )\n",
    "    faster_rcnn = FasterRCNNVGG16()\n",
    "    print('model construct completed')\n",
    "    trainer = FasterRCNNTrainer(faster_rcnn).cuda()\n",
    "    if opt.load_path:\n",
    "        trainer.load(opt.load_path)\n",
    "        print('load pretrained model from %s' % opt.load_path)\n",
    "\n",
    "    trainer.vis.text(dataset.db.label_names, win='labels')\n",
    "    best_map = 0\n",
    "    lr_ = opt.lr\n",
    "    for epoch in range(opt.epoch):\n",
    "        trainer.reset_meters()\n",
    "        for ii, (img, bbox_, label_, scale) in tqdm(enumerate(dataloader)):\n",
    "            scale = at.scalar(scale)\n",
    "            img, bbox, label = img.cuda().float(), bbox_.cuda(), label_.cuda()\n",
    "            img, bbox, label = Variable(img), Variable(bbox), Variable(label)\n",
    "            trainer.train_step(img, bbox, label, scale)\n",
    "\n",
    "            if (ii + 1) % opt.plot_every == 0:\n",
    "                if os.path.exists(opt.debug_file):\n",
    "                    ipdb.set_trace()\n",
    "\n",
    "                # plot loss\n",
    "                trainer.vis.plot_many(trainer.get_meter_data())\n",
    "\n",
    "                # plot groud truth bboxes\n",
    "                ori_img_ = inverse_normalize(at.tonumpy(img[0]))\n",
    "                gt_img = visdom_bbox(ori_img_,\n",
    "                                     at.tonumpy(bbox_[0]),\n",
    "                                     at.tonumpy(label_[0]))\n",
    "                trainer.vis.img('gt_img', gt_img)\n",
    "\n",
    "                # plot predicti bboxes\n",
    "                _bboxes, _labels, _scores = trainer.faster_rcnn.predict([ori_img_], visualize=True)\n",
    "                pred_img = visdom_bbox(ori_img_,\n",
    "                                       at.tonumpy(_bboxes[0]),\n",
    "                                       at.tonumpy(_labels[0]).reshape(-1),\n",
    "                                       at.tonumpy(_scores[0]))\n",
    "                trainer.vis.img('pred_img', pred_img)\n",
    "\n",
    "                # rpn confusion matrix(meter)\n",
    "                trainer.vis.text(str(trainer.rpn_cm.value().tolist()), win='rpn_cm')\n",
    "                # roi confusion matrix\n",
    "                trainer.vis.img('roi_cm', at.totensor(trainer.roi_cm.conf, False).float())\n",
    "        eval_result = eval(test_dataloader, faster_rcnn, test_num=opt.test_num)\n",
    "\n",
    "        if eval_result['map'] > best_map:\n",
    "            best_map = eval_result['map']\n",
    "            best_path = trainer.save(best_map=best_map)\n",
    "        if epoch == 9:\n",
    "            trainer.load(best_path)\n",
    "            trainer.faster_rcnn.scale_lr(opt.lr_decay)\n",
    "            lr_ = lr_ * opt.lr_decay\n",
    "\n",
    "        trainer.vis.plot('test_map', eval_result['map'])\n",
    "        log_info = 'lr:{}, map:{},loss:{}'.format(str(lr_),\n",
    "                                                  str(eval_result['map']),\n",
    "                                                  str(trainer.get_meter_data()))\n",
    "        trainer.vis.log(log_info)\n",
    "        if epoch == 13: \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n",
      "model construct completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1759it [05:14,  5.59it/s]\n",
      "154it [00:11, 13.03it/s]\n",
      "1759it [05:17,  5.54it/s]\n",
      "154it [00:11, 13.21it/s]\n",
      "2it [00:00,  2.96it/s]Process Process-33:\n",
      "Process Process-34:\n",
      "Process Process-40:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/root/briansp/git/simple-faster-rcnn-pytorch/data/dataset.py\", line 107, in __getitem__\n",
      "    img, bbox, label, scale = self.tsf((ori_img, bbox, label))\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/root/briansp/git/simple-faster-rcnn-pytorch/data/dataset.py\", line 105, in __getitem__\n",
      "    ori_img, bbox, label, difficult = self.db.get_example(idx)\n",
      "  File \"/root/briansp/git/simple-faster-rcnn-pytorch/data/dataset.py\", line 107, in __getitem__\n",
      "    img, bbox, label, scale = self.tsf((ori_img, bbox, label))\n",
      "  File \"/root/briansp/git/simple-faster-rcnn-pytorch/data/dataset.py\", line 84, in __call__\n",
      "    img = preprocess(img, self.min_size, self.max_size)\n",
      "  File \"/root/briansp/git/simple-faster-rcnn-pytorch/data/nuclei_dataset.py\", line 125, in get_example\n",
      "    mask_img = cv2.imread(mask_filepath, 0)\n",
      "  File \"/root/briansp/git/simple-faster-rcnn-pytorch/data/dataset.py\", line 65, in preprocess\n",
      "    img = sktsf.resize(img, (C, H * scale, W * scale), mode='reflect')\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py\", line 103, in resize\n",
      "    map_rows, map_cols, map_dims = np.mgrid[:rows, :cols, :dim]\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/numpy/lib/index_tricks.py\", line 199, in __getitem__\n",
      "    nn[k] = (nn[k]*step+start)\n",
      "KeyboardInterrupt\n",
      "Process Process-39:\n",
      "  File \"/root/briansp/git/simple-faster-rcnn-pytorch/data/dataset.py\", line 84, in __call__\n",
      "    img = preprocess(img, self.min_size, self.max_size)\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/briansp/git/simple-faster-rcnn-pytorch/data/dataset.py\", line 65, in preprocess\n",
      "    img = sktsf.resize(img, (C, H * scale, W * scale), mode='reflect')\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py\", line 105, in resize\n",
      "    map_cols = col_scale * (map_cols + 0.5) - 0.5\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/root/briansp/git/simple-faster-rcnn-pytorch/data/dataset.py\", line 107, in __getitem__\n",
      "    img, bbox, label, scale = self.tsf((ori_img, bbox, label))\n",
      "  File \"/root/briansp/git/simple-faster-rcnn-pytorch/data/dataset.py\", line 84, in __call__\n",
      "    img = preprocess(img, self.min_size, self.max_size)\n",
      "  File \"/root/briansp/git/simple-faster-rcnn-pytorch/data/dataset.py\", line 65, in preprocess\n",
      "    img = sktsf.resize(img, (C, H * scale, W * scale), mode='reflect')\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py\", line 113, in resize\n",
      "    mode=ndi_mode, cval=cval)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 343, in map_coordinates\n",
      "    output, order, mode, cval, None, None)\n",
      "KeyboardInterrupt\n",
      "Process Process-35:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/root/briansp/git/simple-faster-rcnn-pytorch/data/dataset.py\", line 107, in __getitem__\n",
      "    img, bbox, label, scale = self.tsf((ori_img, bbox, label))\n",
      "  File \"/root/briansp/git/simple-faster-rcnn-pytorch/data/dataset.py\", line 84, in __call__\n",
      "    img = preprocess(img, self.min_size, self.max_size)\n",
      "  File \"/root/briansp/git/simple-faster-rcnn-pytorch/data/dataset.py\", line 65, in preprocess\n",
      "    img = sktsf.resize(img, (C, H * scale, W * scale), mode='reflect')\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py\", line 113, in resize\n",
      "    mode=ndi_mode, cval=cval)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 343, in map_coordinates\n",
      "    output, order, mode, cval, None, None)\n",
      "KeyboardInterrupt\n",
      "\n",
      "Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7f04032fd860>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 333, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 319, in _shutdown_workers\n",
      "    self.data_queue.get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 487, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 614, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-eec146981a61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNucleiConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-02a998f49f9f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mii\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/briansp/git/simple-faster-rcnn-pytorch/trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, imgs, bboxes, labels, scale)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/briansp/git/simple-faster-rcnn-pytorch/trainer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, imgs, bboxes, labels, scale)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# ------------------ RPN losses -------------------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         gt_rpn_loc, gt_rpn_label = self.anchor_target_creator(\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtonumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0manchor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             img_size)\n",
      "\u001b[0;32m~/briansp/git/simple-faster-rcnn-pytorch/utils/array_tool.py\u001b[0m in \u001b[0;36mtonumpy\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtonumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/briansp/git/simple-faster-rcnn-pytorch/utils/array_tool.py\u001b[0m in \u001b[0;36mtonumpy\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TensorBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtonumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;34mr\"\"\"Returns a CPU copy of this tensor if it's not already on the CPU\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0m__new__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class NucleiConfig(Config):\n",
    "    env = 'nuclei-rcnn'  # visdom env\n",
    "    \n",
    "opt = NucleiConfig()\n",
    "\n",
    "train(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to download pretrained model from [google dirve](https://drive.google.com/open?id=1cQ27LIn-Rig4-Uayzy_gH5-cW-NRGVzY) \n",
    "# 1. model converted from chainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# in this machine the cupy isn't install correctly... \n",
    "# so it's a little slow\n",
    "trainer.load('chainer_best_model_converted_to_pytorch_0.7053.pth')\n",
    "opt.caffe_pretrain=True # this model was trained from caffe-pretrained model\n",
    "_bboxes, _labels, _scores = trainer.faster_rcnn.predict(img,visualize=True)\n",
    "vis_bbox(at.tonumpy(img[0]),\n",
    "         at.tonumpy(_bboxes[0]),\n",
    "         at.tonumpy(_labels[0]).reshape(-1),\n",
    "         at.tonumpy(_scores[0]).reshape(-1))\n",
    "# it failed to find the dog, but if you set threshold from 0.7 to 0.6, you'll find it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. model trained with torchvision pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load('fasterrcnn_12211511_0.701052458187_torchvision_pretrain.pth')\n",
    "opt.caffe_pretrain=False # this model was trained from torchvision-pretrained model\n",
    "_bboxes, _labels, _scores = trainer.faster_rcnn.predict(img,visualize=True)\n",
    "vis_bbox(at.tonumpy(img[0]),\n",
    "         at.tonumpy(_bboxes[0]),\n",
    "         at.tonumpy(_labels[0]).reshape(-1),\n",
    "         at.tonumpy(_scores[0]).reshape(-1))\n",
    "# it failed to find the dog, but if you set threshold from 0.7 to 0.6, you'll find it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. model trained with caffe pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load('fasterrcnn_12222105_0.712649824453_caffe_pretrain.pth')\n",
    "opt.caffe_pretrain=True # this model was trained from caffe-pretrained model\n",
    "_bboxes, _labels, _scores = trainer.faster_rcnn.predict(img,visualize=True)\n",
    "vis_bbox(at.tonumpy(img[0]),\n",
    "         at.tonumpy(_bboxes[0]),\n",
    "         at.tonumpy(_labels[0]).reshape(-1),\n",
    "         at.tonumpy(_scores[0]).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
